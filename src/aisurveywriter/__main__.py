import os
import argparse
import re

from aisurveywriter import generate_paper_survey
from aisurveywriter.utils import get_all_files_from_paths

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("references_dir", help="Path to directory containg all PDF references")
    parser.add_argument("subject", help="Main subject of the survey. Can be the Title too")
    parser.add_argument("--save-dir", type=str, default="./out", help="Path to output directory")
    parser.add_argument("--llm", "-l", choices=["openai", "google"], default="google", help="Specify LLM to use. Either 'google' (gemini-1.5-pro by default) or 'openai' (o1 by default)")
    parser.add_argument("--llm-model", "-m", dest="llm_model", default="gemini-2.0-flash", help="Specific LLM model to use.")
    parser.add_argument("-c", "--config", default="config.yaml", help="YAML file containg your configuration parameters")
    parser.add_argument("--structure", "-s", default=None, type=str, help="YAML file containing the structure to use. If provided, this will skip the structure generation process.")
    parser.add_argument("--paper", "-p", default=None, help="Path to .TEX paper to use. If provided, won't write one from the structure, and will skip directly to reviewing it (unless --no-review) is provided")
    parser.add_argument("--no-review", action="store_true", help="Skip content/writing review step")
    parser.add_argument("--nblm", action="store_true", help="Use NotebookLM for generating the structure")
    parser.add_argument("--embed-model", "-e", default="sentence-transformers/all-MiniLM-L6-v2", help="Text embedding model name")
    parser.add_argument("--embed-type", "-t", default="huggingface", help="Text embedding model type (google, openai, huggingface)")
    parser.add_argument("--bibdb", "-b", type=str, default=None, help="Path to .bib database to use. If none is provided, one will be generated by extracting every reference across all PDFs")
    parser.add_argument("--faissbib", "-fb", type=str, default=None, help="Path to FAISS vector store of the .bib databse. If none is provided, one will be generated")
    parser.add_argument("--images", "-i", type=str, default=None, help="Path to all images extracted from the PDFs. If none is provided, all images will be extracted and saved to a temporary folder")
    parser.add_argument("--faissfig", "-ff", type=str, default=None, help="Path to FAISS vector store containing the metadata (id, path and description) for every image. If none is provided, one will be created")
    parser.add_argument("--faissref", "-fr", action="store_true", help="Use FAISS of references PDFs to retrieve only a piece of information, instead of sending the entire document.")
    parser.add_argument("--no-figures", action="store_true", help="Skip step of adding figures to the written paper.")
    parser.add_argument("--cooldown", "-w", type=int, default=30, help="Cooldown between two consecutive requests made to the LLM API")
    parser.add_argument("--embed-cooldown", type=int, default=0, help="Cooldown between two consecutive requests made to the text embedding model API")

    return parser.parse_args()

def main():
    args = parse_args()

    generate_paper_survey(
        subject=args.subject,
        ref_paths=get_all_files_from_paths(args.references_dir),
        save_path=os.path.abspath(args.save_dir),
        model=args.llm_model,
        model_type=args.llm,
        pregen_struct_yaml=args.structure,
        prewritten_paper_tex=args.paper,
        use_ref_faiss=args.faissref,
        no_review=args.no_review,
        config_path=args.config,
        use_nblm_generation=args.nblm,
        bibdb_path=args.bibdb,
        faissbibdb_path=args.faissbib,
        faissfig_path=args.faissfig,
        imgs_path=os.path.abspath(args.images),
        embed_model=args.embed_model,
        embed_model_type=args.embed_type,
        request_cooldown_sec=args.cooldown,
        embed_request_cooldown_sec=args.embed_cooldown,
        no_figures=args.no_figures,
    )

if __name__ == "__main__":
    main()