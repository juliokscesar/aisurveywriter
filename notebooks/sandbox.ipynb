{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====> ENVIRONMENT SETUP\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "def read_yaml(fpath: str) -> dict:\n",
    "    with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return data\n",
    "\n",
    "CREDENTIALS = read_yaml(\"../credentials.yaml\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = CREDENTIALS[\"google_key\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = CREDENTIALS[\"openai_key\"]\n",
    "os.environ[\"NBLM_EMAIL\"] = CREDENTIALS[\"nblm_email\"]\n",
    "os.environ[\"NBLM_PASSWORD\"] = CREDENTIALS[\"nblm_password\"]\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "llm.invoke([HumanMessage(content=\"Write a poem about the moon\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeling testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pdf data from references\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_files = [\n",
    "    \"../refexamples/ArigaK2023_Langmuir.pdf\",\n",
    "    \"../refexamples/FangC_ApplicationsLangmuir.pdf\",\n",
    "]\n",
    "doc_data = []\n",
    "for file in pdf_files:\n",
    "    loader = PyPDFLoader(file)\n",
    "    doc_data.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "with open(\"../paper_instructions.yaml\", \"r\") as f:\n",
    "    paper = yaml.safe_load(f)\n",
    "\n",
    "pdf_data = \"\\n\".join([doc.page_content for doc in doc_data])\n",
    "prompt_fmt = paper[\"base_prompt_format\"] + \"\\n\\nThe accompanying PDF data for the references is:\\n{pdf_data}\"\n",
    "prep_instructions = paper[\"preparation_instructions\"]\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"preparation_instructions\", \"title\", \"subject\", \"description\"],\n",
    "    template=paper[\"base_prompt_format\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "gen_sections = []\n",
    "\n",
    "for section in paper[\"sections\"]:\n",
    "    response = chain.run({\n",
    "        \"preparation_instructions\": prep_instructions,\n",
    "        \"subject\": paper[\"subject\"],\n",
    "        \"pdf_data\": pdf_data,\n",
    "        \"title\": section[\"title\"],\n",
    "        \"description\": section[\"description\"],\n",
    "    })\n",
    "    gen_sections.append({\"title\": section[\"title\"], \"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gen_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = {\"sections\": gen_sections}\n",
    "with open(\"generated20241213\", \"w\") as f:\n",
    "    yaml.dump(dump, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protoyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List,Union\n",
    "import undetected_chromedriver as uc\n",
    "from fake_useragent import UserAgent\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_core.messages import SystemMessage, AIMessage\n",
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from chatbots import NotebookLMBot\n",
    "\n",
    "import re\n",
    "import os\n",
    "from time import sleep\n",
    "\n",
    "def init_driver(browser_path: Union[str,None] = None, driver_path: Union[str,None] = None) -> uc.Chrome:\n",
    "    op = uc.ChromeOptions()\n",
    "    op.add_argument(f\"user-agent={UserAgent.random}\")\n",
    "    op.add_argument(\"user-data-dir=./\")\n",
    "    op.add_experimental_option(\"detach\", True)\n",
    "    op.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "    driver = uc.Chrome(\n",
    "            chrome_options=op,\n",
    "            browser_executable_path=browser_path,\n",
    "            driver_executable_path=driver_path\n",
    "    )\n",
    "    return driver\n",
    "\n",
    "def get_pdf_contents(pdf_paths: List[str]):\n",
    "    doc_data = []\n",
    "    for file in pdf_paths:\n",
    "        loader = PyPDFLoader(file)\n",
    "        doc_data.extend(loader.load())\n",
    "    return doc_data\n",
    "\n",
    "def generate_paper_structure(prompt: str, subject: str, pdf_paths: List[str], outfile: str):\n",
    "    \"\"\" Generate paper structure using NotebookLM \"\"\"\n",
    "    if prompt.find(\"{subject}\") != -1:\n",
    "        prompt = prompt.replace(\"{subject}\", subject)\n",
    "\n",
    "    # Use NotebookLM bot to send it\n",
    "    driver = init_driver(None, \"../drivers/chromedriver\")\n",
    "    nblm = NotebookLMBot(\n",
    "        user=os.environ[\"NBLM_EMAIL\"],\n",
    "        password=os.environ[\"NBLM_PASSWORD\"],\n",
    "        driver=driver,\n",
    "        src_paths=pdf_paths\n",
    "    )\n",
    "    if not nblm.login():\n",
    "        print(\"Unable to login to NotebookLM\")\n",
    "        return\n",
    "    \n",
    "    nblm.send_prompt(prompt, sleep_for=30)\n",
    "    response = nblm.get_last_response()\n",
    "\n",
    "    # format response and save to yaml\n",
    "    result = \"sections:\\n\"\n",
    "    for line in response.split(\"\\n\"):\n",
    "        result += f\"  {line}\\n\"\n",
    "    \n",
    "    with open(outfile, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(result)\n",
    "\n",
    "    driver.quit()\n",
    "    return read_yaml(outfile)\n",
    "\n",
    "def setup_context_msg(response_fmt_prompt: str, pdf_paths: List[str]):\n",
    "    \"\"\" Setup context SystemMessage with writing instructions + PDFs contents \"\"\"\n",
    "    \n",
    "    context = response_fmt_prompt\n",
    "    pdf_content = get_pdf_contents(pdf_paths)\n",
    "    context += \"\\n\\nThe PDF content of the given references are:\\n\"\n",
    "    context += \"\\n\".join([doc.page_content for doc in pdf_content])\n",
    "\n",
    "    return SystemMessage(content=context)\n",
    "\n",
    "def init_chain(llm, ctx_msg: SystemMessage, write_prompt: str):\n",
    "    \"\"\" Setup LLMChain with proper prompts and context \"\"\"\n",
    "    req_prompt = HumanMessagePromptTemplate.from_template(write_prompt)\n",
    "    \n",
    "    full_prompt = ChatPromptTemplate.from_messages([ctx_msg, req_prompt])\n",
    "    chain = full_prompt | llm\n",
    "    return full_prompt, chain\n",
    "\n",
    "def write_section(chain, subject: str, title: str, description: str) -> AIMessage:\n",
    "    \"\"\" Write the given section \"\"\"\n",
    "    return chain.invoke({\n",
    "        \"subject\": subject,\n",
    "        \"title\": title,\n",
    "        \"description\": description,\n",
    "    })\n",
    "\n",
    "def dump_generated_sections(sections: dict, outpath: str):\n",
    "    with open(outpath, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.safe_dump(sections, f)\n",
    "\n",
    "\n",
    "def save_latex_sections(tex_template_path: str, sections: List[dict], outpath: str):\n",
    "    \"\"\" \n",
    "    Join the contents of every section to the output LaTeX file \n",
    "    'sections' must be a list of dictionaries with two keys: 'title' and 'content'\n",
    "    \"\"\"\n",
    "    with open(tex_template_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        tex_template = f.read()\n",
    "\n",
    "    paper_content = \"\"\n",
    "    \n",
    "    bib_content = \"\"\n",
    "    bib_pattern = r\"\\\\begin{filecontents\\*}(.*?)\\\\end{filecontents\\*}\"\n",
    "\n",
    "    for section in sections:\n",
    "        # Extract biblatex file content\n",
    "        match = re.search(bib_pattern, section[\"content\"], re.DOTALL)\n",
    "        sec_bib_content = match.group(1).strip() if match else None\n",
    "        if sec_bib_content is None:\n",
    "            print(\"FAILED TO MATCH BIBLATEX CONTENT IN SECTION:\", section[\"title\"])\n",
    "            continue\n",
    "\n",
    "        section_text = re.sub(bib_pattern, \"\", section[\"content\"], flags=re.DOTALL)\n",
    "        \n",
    "        paper_content += section_text\n",
    "        bib_content += sec_bib_content\n",
    "    bib_content = bib_content.replace(\"{mybib.bib}\", \"\")\n",
    "    bib_file = outpath+\"bib.bib\"\n",
    "    \n",
    "    # Replace paper content in latex template and save it\n",
    "    tex_content = tex_template.replace(\"{content}\", paper_content).replace(\"{bibresourcefile}\", os.path.basename(bib_file))\n",
    "    with open(outpath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(tex_content)\n",
    "\n",
    "    # also save the biblatex file\n",
    "    with open(bib_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(bib_content)\n",
    "\n",
    "\n",
    "def main():\n",
    "    paper_cfg = read_yaml(\"../templates/prompt_config.yaml\")\n",
    "    pdf_paths = [\n",
    "        \"../refexamples/ArigaK2023_Langmuir.pdf\",\n",
    "        \"../refexamples/FangC_ApplicationsLangmuir.pdf\",\n",
    "        \"../refexamples/ArigaK2022_PastAndFutureLangmuir.pdf\",\n",
    "        # \"../refexamples/LuC2024_AIScientist.pdf\"\n",
    "    ]\n",
    "    paper_subject = paper_cfg[\"subject\"]\n",
    "    # paper_structure = generate_paper_structure(\n",
    "    #     prompt=paper_cfg[\"gen_struct_prompt\"],\n",
    "    #     subject=paper_subject,\n",
    "    #     pdf_paths=pdf_paths,\n",
    "    #     outfile=\"genstruct.yaml\"\n",
    "    # )\n",
    "    paper_structure = read_yaml(\"genstruct.yaml\")\n",
    "\n",
    "\n",
    "    ctx = setup_context_msg(\n",
    "        response_fmt_prompt=paper_cfg[\"response_format\"],\n",
    "        pdf_paths=pdf_paths,\n",
    "    )\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "    prompt, chain = init_chain(llm, ctx, paper_cfg[\"write_prompt\"])\n",
    "\n",
    "    paper_content = []\n",
    "    for section in paper_structure[\"sections\"]:\n",
    "        airesponse = write_section(chain, paper_subject, section[\"title\"], section[\"description\"])\n",
    "        paper_content.append(\n",
    "            {\n",
    "                \"title\": section[\"title\"],\n",
    "                \"content\": airesponse.content,\n",
    "            }\n",
    "        )\n",
    "        print(\"====> FINISHED WRITING SECTION:\", section[\"title\"])\n",
    "        print(\"====> REPONSE METADATA:\", airesponse.usage_metadata)\n",
    "        # wait because of gemini-1.5-pro quota (2 RPM, 32000 TPM)\n",
    "        sleep(60*1.5)\n",
    "\n",
    "    os.makedirs(\"out\", exist_ok=True)\n",
    "    dump_generated_sections({\"sections\": paper_content}, \"out/lastgeneration.dump\")\n",
    "    save_latex_sections(\n",
    "        tex_template_path=\"../templates/paper_template.tex\",\n",
    "        sections=paper_content,\n",
    "        outpath=\"out/lastgenerated.tex\",\n",
    "    )\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([sec[\"title\"] for sec in read_yaml(\"genstruct.yaml\")[\"sections\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
