{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====> ENVIRONMENT SETUP\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "def read_yaml(fpath: str) -> dict:\n",
    "    with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return data\n",
    "\n",
    "CREDENTIALS = read_yaml(\"../credentials.yaml\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = CREDENTIALS[\"google_key\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = CREDENTIALS[\"openai_key\"]\n",
    "os.environ[\"NBLM_EMAIL\"] = CREDENTIALS[\"nblm_email\"]\n",
    "os.environ[\"NBLM_PASSWORD\"] = CREDENTIALS[\"nblm_password\"]\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "llm.invoke([HumanMessage(content=\"Write a poem about the moon\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeling testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pdf data from references\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_files = [\n",
    "    \"../refexamples/ArigaK2023_Langmuir.pdf\",\n",
    "    \"../refexamples/FangC_ApplicationsLangmuir.pdf\",\n",
    "]\n",
    "doc_data = []\n",
    "for file in pdf_files:\n",
    "    loader = PyPDFLoader(file)\n",
    "    doc_data.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "with open(\"../paper_instructions.yaml\", \"r\") as f:\n",
    "    paper = yaml.safe_load(f)\n",
    "\n",
    "pdf_data = \"\\n\".join([doc.page_content for doc in doc_data])\n",
    "prompt_fmt = paper[\"base_prompt_format\"] + \"\\n\\nThe accompanying PDF data for the references is:\\n{pdf_data}\"\n",
    "prep_instructions = paper[\"preparation_instructions\"]\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"preparation_instructions\", \"title\", \"subject\", \"description\"],\n",
    "    template=paper[\"base_prompt_format\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "gen_sections = []\n",
    "\n",
    "for section in paper[\"sections\"]:\n",
    "    response = chain.run({\n",
    "        \"preparation_instructions\": prep_instructions,\n",
    "        \"subject\": paper[\"subject\"],\n",
    "        \"pdf_data\": pdf_data,\n",
    "        \"title\": section[\"title\"],\n",
    "        \"description\": section[\"description\"],\n",
    "    })\n",
    "    gen_sections.append({\"title\": section[\"title\"], \"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gen_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = {\"sections\": gen_sections}\n",
    "with open(\"generated20241213\", \"w\") as f:\n",
    "    yaml.dump(dump, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protoyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> STARTED WRITING SECTION: Introduction\n",
      "====> FINISHED WRITING SECTION: Introduction\n",
      "====> REPONSE METADATA: {'input_tokens': 8890, 'output_tokens': 9928, 'total_tokens': 18818, 'input_token_details': {'audio': 0, 'cache_read': 8704}, 'output_token_details': {'audio': 0, 'reasoning': 6656}} \n",
      "\n",
      "====> STARTED WRITING SECTION: Basic Concepts of Langmuir Monolayers and Langmuir-Blodgett Films\n",
      "====> FINISHED WRITING SECTION: Basic Concepts of Langmuir Monolayers and Langmuir-Blodgett Films\n",
      "====> REPONSE METADATA: {'input_tokens': 8806, 'output_tokens': 2890, 'total_tokens': 11696, 'input_token_details': {'audio': 0, 'cache_read': 8704}, 'output_token_details': {'audio': 0, 'reasoning': 1536}} \n",
      "\n",
      "====> STARTED WRITING SECTION: Characterization Techniques for Langmuir Monolayers and LB Films\n",
      "====> FINISHED WRITING SECTION: Characterization Techniques for Langmuir Monolayers and LB Films\n",
      "====> REPONSE METADATA: {'input_tokens': 7882, 'output_tokens': 5178, 'total_tokens': 13060, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1536}} \n",
      "\n",
      "====> STARTED WRITING SECTION: Fabrication of LB Films from Various Materials\n",
      "====> FINISHED WRITING SECTION: Fabrication of LB Films from Various Materials\n",
      "====> REPONSE METADATA: {'input_tokens': 7284, 'output_tokens': 6569, 'total_tokens': 13853, 'input_token_details': {'audio': 0, 'cache_read': 7168}, 'output_token_details': {'audio': 0, 'reasoning': 4032}} \n",
      "\n",
      "====> STARTED WRITING SECTION: Advanced Langmuir-Blodgett Methods\n",
      "====> FINISHED WRITING SECTION: Advanced Langmuir-Blodgett Methods\n",
      "====> REPONSE METADATA: {'input_tokens': 8778, 'output_tokens': 5568, 'total_tokens': 14346, 'input_token_details': {'audio': 0, 'cache_read': 8704}, 'output_token_details': {'audio': 0, 'reasoning': 3712}} \n",
      "\n",
      "====> STARTED WRITING SECTION: Layer-by-Layer (LbL) Assembly\n",
      "====> FINISHED WRITING SECTION: Layer-by-Layer (LbL) Assembly\n",
      "====> REPONSE METADATA: {'input_tokens': 8636, 'output_tokens': 7511, 'total_tokens': 16147, 'input_token_details': {'audio': 0, 'cache_read': 8448}, 'output_token_details': {'audio': 0, 'reasoning': 4800}} \n",
      "\n",
      "====> STARTED WRITING SECTION: Applications of Langmuir-Blodgett Films\n",
      "====> FINISHED WRITING SECTION: Applications of Langmuir-Blodgett Films\n",
      "====> REPONSE METADATA: {'input_tokens': 8998, 'output_tokens': 6965, 'total_tokens': 15963, 'input_token_details': {'audio': 0, 'cache_read': 8320}, 'output_token_details': {'audio': 0, 'reasoning': 4800}} \n",
      "\n",
      "====> STARTED WRITING SECTION: Interfacial Nanoarchitectonics with Langmuir-Blodgett Films\n",
      "====> FINISHED WRITING SECTION: Interfacial Nanoarchitectonics with Langmuir-Blodgett Films\n",
      "====> REPONSE METADATA: {'input_tokens': 8904, 'output_tokens': 6676, 'total_tokens': 15580, 'input_token_details': {'audio': 0, 'cache_read': 4096}, 'output_token_details': {'audio': 0, 'reasoning': 5568}} \n",
      "\n",
      "====> STARTED WRITING SECTION: Future Directions and Challenges\n",
      "====> FINISHED WRITING SECTION: Future Directions and Challenges\n",
      "====> REPONSE METADATA: {'input_tokens': 8842, 'output_tokens': 6819, 'total_tokens': 15661, 'input_token_details': {'audio': 0, 'cache_read': 6912}, 'output_token_details': {'audio': 0, 'reasoning': 4928}} \n",
      "\n",
      "====> STARTED WRITING SECTION: Conclusions\n",
      "====> FINISHED WRITING SECTION: Conclusions\n",
      "====> REPONSE METADATA: {'input_tokens': 8772, 'output_tokens': 4572, 'total_tokens': 13344, 'input_token_details': {'audio': 0, 'cache_read': 1280}, 'output_token_details': {'audio': 0, 'reasoning': 2880}} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List,Union\n",
    "import undetected_chromedriver as uc\n",
    "from fake_useragent import UserAgent\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_core.messages import SystemMessage, AIMessage\n",
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from chatbots import NotebookLMBot\n",
    "\n",
    "import re\n",
    "import os\n",
    "from time import sleep\n",
    "\n",
    "def init_driver(browser_path: Union[str,None] = None, driver_path: Union[str,None] = None) -> uc.Chrome:\n",
    "    op = uc.ChromeOptions()\n",
    "    op.add_argument(f\"user-agent={UserAgent.random}\")\n",
    "    op.add_argument(\"user-data-dir=./\")\n",
    "    op.add_experimental_option(\"detach\", True)\n",
    "    op.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "    driver = uc.Chrome(\n",
    "            chrome_options=op,\n",
    "            browser_executable_path=browser_path,\n",
    "            driver_executable_path=driver_path\n",
    "    )\n",
    "    return driver\n",
    "\n",
    "def get_pdf_contents(pdf_paths: List[str]):\n",
    "    doc_data = []\n",
    "    for file in pdf_paths:\n",
    "        loader = PyPDFLoader(file)\n",
    "        doc_data.extend(loader.load())\n",
    "    return doc_data\n",
    "\n",
    "def generate_paper_structure(prompt: str, subject: str, pdf_paths: List[str], outfile: str, driver_path: str = \"../drivers/chromedriver\", browser_path: Union[str,None] = None):\n",
    "    \"\"\" Generate paper structure using NotebookLM \"\"\"\n",
    "    if prompt.find(\"{subject}\") != -1:\n",
    "        prompt = prompt.replace(\"{subject}\", subject)\n",
    "\n",
    "    # Use NotebookLM bot to send it\n",
    "    driver = init_driver(browser_path, driver_path)\n",
    "    nblm = NotebookLMBot(\n",
    "        user=os.environ[\"NBLM_EMAIL\"],\n",
    "        password=os.environ[\"NBLM_PASSWORD\"],\n",
    "        driver=driver,\n",
    "        src_paths=pdf_paths\n",
    "    )\n",
    "    if not nblm.login():\n",
    "        print(\"Unable to login to NotebookLM\")\n",
    "        return\n",
    "    \n",
    "    nblm.send_prompt(prompt, sleep_for=30)\n",
    "    response = nblm.get_last_response()\n",
    "\n",
    "    # format response and save to yaml\n",
    "    result = \"sections:\\n\"\n",
    "    for line in response.split(\"\\n\"):\n",
    "        result += f\"  {line}\\n\"\n",
    "    \n",
    "    with open(outfile, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(result)\n",
    "\n",
    "    result = read_yaml(outfile)\n",
    "    print(f\"Finished generating paper structure. Got a structure with {len(result[\"sections\"])} sections.\\n\")\n",
    "\n",
    "    driver.quit()\n",
    "    return read_yaml(outfile)\n",
    "\n",
    "def setup_context_msg(response_fmt_prompt: str, pdf_paths: List[str], summary_llm):\n",
    "    \"\"\" Setup context SystemMessage with writing instructions + PDFs contents \"\"\"\n",
    "    \n",
    "    context = response_fmt_prompt\n",
    "    pdf_content = get_pdf_contents(pdf_paths)\n",
    "\n",
    "    # Summarize pdf contents\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_documents(pdf_content)\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        summary = summary_llm(f\"Summarize this text: {chunk}\")\n",
    "        sleep(1*60)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    context += \"\\n\\nThe PDF content of the given references are:\\n\"\n",
    "    context += \"\\n\".join(summary)\n",
    "\n",
    "    return SystemMessage(content=context)\n",
    "\n",
    "def vector_store_from_pdf_content(pdf_content):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_documents(pdf_content)\n",
    "    chunks = [chunk.page_content for chunk in chunks]\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vector_store = FAISS.from_texts(chunks, embeddings)\n",
    "    return vector_store\n",
    "\n",
    "def setup_ctx_msg_faiss(response_fmt_prompt: str, vector_store: FAISS, section_query: str):\n",
    "    context = response_fmt_prompt\n",
    "    \n",
    "    relevant_docs = vector_store.similarity_search(section_query, k=4)\n",
    "    context += \"\\n\"\n",
    "    context += \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    return context\n",
    "\n",
    "def init_chain(llm, ctx_msg: SystemMessage, write_prompt: str):\n",
    "    \"\"\" Setup LLMChain with proper prompts and context \"\"\"\n",
    "    req_prompt = HumanMessagePromptTemplate.from_template(write_prompt)\n",
    "    \n",
    "    full_prompt = ChatPromptTemplate.from_messages([ctx_msg, req_prompt])\n",
    "    \n",
    "    chain = full_prompt | llm\n",
    "    return full_prompt, chain\n",
    "\n",
    "def write_section(chain, subject: str, title: str, description: str) -> AIMessage:\n",
    "    \"\"\" Write the given section \"\"\"\n",
    "    return chain.invoke({\n",
    "        \"subject\": subject,\n",
    "        \"title\": title,\n",
    "        \"description\": description,\n",
    "    })\n",
    "\n",
    "def dump_generated_sections(sections: dict, outpath: str):\n",
    "    with open(outpath, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.safe_dump(sections, f)\n",
    "\n",
    "\n",
    "def save_latex_sections(tex_template_path: str, sections: List[dict], outpath: str):\n",
    "    \"\"\" \n",
    "    Join the contents of every section to the output LaTeX file \n",
    "    'sections' must be a list of dictionaries with two keys: 'title' and 'content'\n",
    "    \"\"\"\n",
    "    with open(tex_template_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        tex_template = f.read()\n",
    "\n",
    "    paper_content = \"\"\n",
    "    \n",
    "    bib_content = \"\"\n",
    "    bib_pattern = r\"\\\\begin{filecontents\\*}(.*?)\\\\end{filecontents\\*}\"\n",
    "\n",
    "    for section in sections:\n",
    "        # Extract biblatex file content\n",
    "        match = re.search(bib_pattern, section[\"content\"], re.DOTALL)\n",
    "        sec_bib_content = match.group(1).strip() if match else None\n",
    "        if sec_bib_content is None:\n",
    "            print(\"FAILED TO MATCH BIBLATEX CONTENT IN SECTION:\", section[\"title\"])\n",
    "            continue\n",
    "\n",
    "        section_text = re.sub(bib_pattern, \"\", section[\"content\"], flags=re.DOTALL)\n",
    "        \n",
    "        paper_content += section_text\n",
    "        bib_content += sec_bib_content\n",
    "    bib_content = bib_content.replace(\"{mybib.bib}\", \"\")\n",
    "    bib_file = outpath+\"bib.bib\"\n",
    "    \n",
    "    # Replace paper content in latex template and save it\n",
    "    tex_content = tex_template.replace(\"{content}\", paper_content).replace(\"{bibresourcefile}\", os.path.basename(bib_file))\n",
    "    tex_content = tex_content.replace(\"```latex\", \"\").replace(\"```\",\"\")\n",
    "    with open(outpath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(tex_content)\n",
    "\n",
    "    # also save the biblatex file\n",
    "    with open(bib_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(bib_content)\n",
    "\n",
    "\n",
    "def main():\n",
    "    paper_cfg = read_yaml(\"../templates/prompt_config.yaml\")\n",
    "    pdf_paths = [\n",
    "        \"../refexamples/ArigaK2023_ChemOfMat.pdf\",\n",
    "        \"../refexamples/FangC2022_LangmuirBattery.pdf\",\n",
    "        \"../refexamples/OliveiraO2022_PastAndFuture.pdf\",\n",
    "        # \"../refexamples/LuC2024_AIScientist.pdf\"\n",
    "    ]\n",
    "    paper_subject = paper_cfg[\"subject\"]\n",
    "\n",
    "    os.makedirs(\"out\", exist_ok=True)\n",
    "\n",
    "    # paper_structure = generate_paper_structure(\n",
    "    #     prompt=paper_cfg[\"gen_struct_prompt\"],\n",
    "    #     subject=paper_subject,\n",
    "    #     pdf_paths=pdf_paths,\n",
    "    #     outfile=\"out/genstruct.yaml\",\n",
    "    #     driver_path=\"../drivers/chromedriver.exe\",\n",
    "    #     browser_path=\"C:/Users/jcmcs/AppData/Local/BraveSoftware/Brave-Browser/Application/brave.exe\",\n",
    "    # )\n",
    "    paper_structure = read_yaml(\"out/genstruct.yaml\")\n",
    "\n",
    "    # ctx = setup_context_msg(\n",
    "    #     response_fmt_prompt=paper_cfg[\"response_format\"],\n",
    "    #     pdf_paths=pdf_paths,\n",
    "    #     summary_llm=ChatOpenAI(model=\"chatgpt-4o-latest\")\n",
    "    # )\n",
    "    llm = ChatOpenAI(model=\"o1-preview\")\n",
    "    vecstore = vector_store_from_pdf_content(\n",
    "        get_pdf_contents(pdf_paths)\n",
    "    )\n",
    "\n",
    "    paper_content = []\n",
    "    for section in paper_structure[\"sections\"]:\n",
    "        print(f\"====> STARTED WRITING SECTION: {section[\"title\"]}\")\n",
    "\n",
    "        ctx = setup_ctx_msg_faiss(\n",
    "            response_fmt_prompt=paper_cfg[\"response_format\"],\n",
    "            vector_store=vecstore,\n",
    "            section_query=f\"Retrieve relevant information for a section called {section['title']} on the subject {paper_subject}. Include any relevant and appropriate references. Do not be concise -- include at least 30% of each paper.\",\n",
    "        )\n",
    "        prompt, chain = init_chain(llm, ctx, paper_cfg[\"write_prompt\"])\n",
    "\n",
    "        airesponse = write_section(chain, paper_subject, section[\"title\"], section[\"description\"])\n",
    "        paper_content.append(\n",
    "            {\n",
    "                \"title\": section[\"title\"],\n",
    "                \"content\": airesponse.content,\n",
    "            }\n",
    "        )\n",
    "        print(\"====> FINISHED WRITING SECTION:\", section[\"title\"])\n",
    "        print(\"====> REPONSE METADATA:\", airesponse.usage_metadata, \"\\n\")\n",
    "        dump_generated_sections({\"sections\": paper_content}, \"out/lastgeneration.dump\")\n",
    "        # wait because of TPM (request quota)\n",
    "        sleep(60*1.5)\n",
    "\n",
    "    save_latex_sections(\n",
    "        tex_template_path=\"../templates/paper_template.tex\",\n",
    "        sections=paper_content,\n",
    "        outpath=\"out/lastgenerated.tex\",\n",
    "    )\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([sec[\"title\"] for sec in read_yaml(\"genstruct.yaml\")[\"sections\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
