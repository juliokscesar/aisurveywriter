{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====> ENVIRONMENT SETUP\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "def read_yaml(fpath: str) -> dict:\n",
    "    with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return data\n",
    "\n",
    "APIKEYS = read_yaml(\"../apikeys.yaml\")\n",
    "os.environ[\"GOOGLE_API_KEY\"]=APIKEYS[\"google\"]\n",
    "os.environ[\"OPENAI_API_KEY\"]=APIKEYS[\"openai\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "llm.invoke([HumanMessage(content=\"Write a poem about the moon\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeling testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pdf data from references\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_files = [\n",
    "    \"../refexamples/ArigaK2023_Langmuir.pdf\",\n",
    "    \"../refexamples/FangC_ApplicationsLangmuir.pdf\",\n",
    "]\n",
    "doc_data = []\n",
    "for file in pdf_files:\n",
    "    loader = PyPDFLoader(file)\n",
    "    doc_data.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "with open(\"../paper_instructions.yaml\", \"r\") as f:\n",
    "    paper = yaml.safe_load(f)\n",
    "\n",
    "pdf_data = \"\\n\".join([doc.page_content for doc in doc_data])\n",
    "prompt_fmt = paper[\"base_prompt_format\"] + \"\\n\\nThe accompanying PDF data for the references is:\\n{pdf_data}\"\n",
    "prep_instructions = paper[\"preparation_instructions\"]\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"preparation_instructions\", \"title\", \"subject\", \"description\"],\n",
    "    template=paper[\"base_prompt_format\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "gen_sections = []\n",
    "\n",
    "for section in paper[\"sections\"]:\n",
    "    response = chain.run({\n",
    "        \"preparation_instructions\": prep_instructions,\n",
    "        \"subject\": paper[\"subject\"],\n",
    "        \"pdf_data\": pdf_data,\n",
    "        \"title\": section[\"title\"],\n",
    "        \"description\": section[\"description\"],\n",
    "    })\n",
    "    gen_sections.append({\"title\": section[\"title\"], \"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gen_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = {\"sections\": gen_sections}\n",
    "with open(\"generated20241213\", \"w\") as f:\n",
    "    yaml.dump(dump, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protoyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julio/Dev/SCG_IFSC/aisurveywriter/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_27410/326096492.py:48: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n",
      "/tmp/ipykernel_27410/326096492.py:56: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return chain.run(title=title,description=description,subject=subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED TO MATCH BIBLATEX CONTENT IN SECTION: Historical Perspective\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain import PromptTemplate, ConversationChain, LLMChain\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import re\n",
    "\n",
    "def get_pdf_contents(pdf_paths: List[str]):\n",
    "    doc_data = []\n",
    "    for file in pdf_paths:\n",
    "        loader = PyPDFLoader(file)\n",
    "        doc_data.extend(loader.load())\n",
    "    return doc_data\n",
    "\n",
    "def generate_paper_structure(prompt: str, subject: str, pdf_paths: List[str]):\n",
    "    \"\"\" Generate paper structure using NotebookLM \"\"\"\n",
    "    if prompt.find(\"{subject}\") != -1:\n",
    "        prompt = prompt.replace(\"{subject}\", subject)\n",
    "\n",
    "    # Use NotebookLM bot to send it\n",
    "    \n",
    "    # Save structure to \"templates/paper_structure.yaml\"\n",
    "    pass\n",
    "\n",
    "def setup_context_msg(preparation_yaml_path: str, pdf_paths: List[str]):\n",
    "    \"\"\" Setup context SystemMessage with writing instructions + PDFs contents \"\"\"\n",
    "    preparation = read_yaml(preparation_yaml_path)\n",
    "    \n",
    "    context = f\"For this context, be aware:\"\n",
    "    for prep in preparation:\n",
    "        context += f\"\\n{preparation[prep]}\"\n",
    "    \n",
    "    pdf_content = get_pdf_contents(pdf_paths)\n",
    "    context += \"\\n\\nThe PDF content of the given references are:\\n\"\n",
    "    context += \"\\n\".join([doc.page_content for doc in pdf_content])\n",
    "\n",
    "    return SystemMessage(content=context)\n",
    "\n",
    "def init_chain(llm, ctx_msg: SystemMessage, write_prompt_yaml: str) -> ConversationChain:\n",
    "    \"\"\" Setup LLMChain with proper prompts and context \"\"\"\n",
    "    prompt_fmt = read_yaml(write_prompt_yaml)\n",
    "\n",
    "    req_prompt = HumanMessagePromptTemplate.from_template(prompt_fmt[\"base_prompt_format\"])\n",
    "    \n",
    "    full_prompt = ChatPromptTemplate.from_messages([ctx_msg, req_prompt])\n",
    "    chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=full_prompt\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "def write_section(chain: ConversationChain, subject: str, title: str, description: str) -> str:\n",
    "    \"\"\" Write the given section \"\"\"\n",
    "    return chain.run(title=title,description=description,subject=subject)\n",
    "\n",
    "def dump_generated_sections(sections: dict, outpath: str):\n",
    "    with open(outpath, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.dump(sections, f)\n",
    "\n",
    "\n",
    "def save_latex_sections(tex_template_path: str, sections: List[dict], outpath: str):\n",
    "    \"\"\" \n",
    "    Join the contents of every section to the output LaTeX file \n",
    "    'sections' must be a list of dictionaries with two keys: 'title' and 'content'\n",
    "    \"\"\"\n",
    "\n",
    "    with open(tex_template_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        tex_template = f.read()\n",
    "\n",
    "    paper_content = \"\"\n",
    "    \n",
    "    bib_content = \"\"\n",
    "    bib_pattern = r\"\\\\begin{filecontents\\*}(.*?)\\\\end{filecontents\\*}\"\n",
    "\n",
    "    for section in sections:\n",
    "        # Extract biblatex file content\n",
    "        match = re.search(bib_pattern, section[\"content\"], re.DOTALL)\n",
    "        sec_bib_content = match.group(1).strip() if match else None\n",
    "        if sec_bib_content is None:\n",
    "            print(\"FAILED TO MATCH BIBLATEX CONTENT IN SECTION:\", section[\"title\"])\n",
    "            continue\n",
    "\n",
    "        section_text = re.sub(bib_pattern, \"\", section[\"content\"], flags=re.DOTALL)\n",
    "        \n",
    "        paper_content += section_text\n",
    "        bib_content += sec_bib_content\n",
    "    \n",
    "    # Replace paper content in latex template and save it\n",
    "    tex_content = tex_template.replace(\"{content}\", paper_content)\n",
    "    with open(outpath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(tex_content)\n",
    "\n",
    "    # also save the biblatex file\n",
    "    with open(outpath+\"bib.bib\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(bib_content)    \n",
    "\n",
    "\n",
    "def main():\n",
    "    pdf_paths = [\n",
    "        \"../refexamples/ArigaK2023_Langmuir.pdf\",\n",
    "        \"../refexamples/FangC_ApplicationsLangmuir.pdf\",\n",
    "    ]\n",
    "    struct_generation = read_yaml(\"../templates/gen_paper_structure.yaml\")\n",
    "    paper_subject = struct_generation[\"subject\"]\n",
    "    # paper_structure = generate_paper_structure(\n",
    "    #     prompt=struct_generation[\"gen_struct_prompt\"],\n",
    "    #     subject=paper_subject,\n",
    "    #     pdf_paths=pdf_paths,\n",
    "    # )\n",
    "\n",
    "    ctx = setup_context_msg(\n",
    "        preparation_yaml_path=\"../templates/preparation.yaml\",\n",
    "        pdf_paths=pdf_paths,\n",
    "    )\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "    chain = init_chain(llm, ctx, \"../templates/write_prompt_fmt.yaml\")\n",
    "    # print(chain.prompt.format_prompt(title=\"Ola\",subject=\"Tchau\",description=\"dadsf\").to_string())\n",
    "\n",
    "    paper_structure = read_yaml(\"../templates/paper_structure.yaml\")\n",
    "    paper_content = []\n",
    "    for section in paper_structure[\"sections\"]:\n",
    "        paper_content.append(\n",
    "            {\n",
    "                \"title\": section[\"title\"],\n",
    "                \"content\": write_section(chain, paper_subject, section[\"title\"], section[\"description\"]),\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    dump_generated_sections({\"sections\": paper_content}, \"lastgeneration.dump\")\n",
    "    save_latex_sections(\n",
    "        tex_template_path=\"../templates/paper_template.tex\",\n",
    "        sections=paper_content,\n",
    "        outpath=\"lastgenerated.tex\",\n",
    "    )\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
