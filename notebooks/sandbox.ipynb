{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====> ENVIRONMENT SETUP\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "def read_yaml(fpath: str) -> dict:\n",
    "    with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return data\n",
    "\n",
    "CREDENTIALS = read_yaml(\"../credentials.yaml\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = CREDENTIALS[\"google_key\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = CREDENTIALS[\"openai_key\"]\n",
    "os.environ[\"NBLM_EMAIL\"] = CREDENTIALS[\"nblm_email\"]\n",
    "os.environ[\"NBLM_PASSWORD\"] = CREDENTIALS[\"nblm_password\"]\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Pale orb in velvet skies,\\nA silent watcher, ever wise.\\nYou hang suspended, pearly bright,\\nA beacon burning through the night.\\n\\nYour craters etched, a timeless map,\\nOf cosmic dust and stellar clap.\\nA silent witness, old and grand,\\nTo shifting sands and changing land.\\n\\nYou pull the tides, a gentle sway,\\nThat shapes the shores, both night and day.\\nA mystic power, soft and deep,\\nWhile slumbering worlds their secrets keep.\\n\\nFrom crescent thin to fullness round,\\nYour changing phases, unbound.\\nA symbol whispered, soft and low,\\nOf dreams and secrets, ebb and flow.\\n\\nAnd when the sun's embrace is gone,\\nYou reign supreme, till break of dawn.\\nA silver disc, a gentle gleam,\\nA poet's muse, a lover's dream.\\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-14339862-3ba9-4ab2-b1ba-ff846774e3c9-0', usage_metadata={'input_tokens': 7, 'output_tokens': 181, 'total_tokens': 188, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "llm.invoke([HumanMessage(content=\"Write a poem about the moon\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeling testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pdf data from references\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_files = [\n",
    "    \"../refexamples/ArigaK2023_Langmuir.pdf\",\n",
    "    \"../refexamples/FangC_ApplicationsLangmuir.pdf\",\n",
    "]\n",
    "doc_data = []\n",
    "for file in pdf_files:\n",
    "    loader = PyPDFLoader(file)\n",
    "    doc_data.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "with open(\"../paper_instructions.yaml\", \"r\") as f:\n",
    "    paper = yaml.safe_load(f)\n",
    "\n",
    "pdf_data = \"\\n\".join([doc.page_content for doc in doc_data])\n",
    "prompt_fmt = paper[\"base_prompt_format\"] + \"\\n\\nThe accompanying PDF data for the references is:\\n{pdf_data}\"\n",
    "prep_instructions = paper[\"preparation_instructions\"]\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"preparation_instructions\", \"title\", \"subject\", \"description\"],\n",
    "    template=paper[\"base_prompt_format\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "gen_sections = []\n",
    "\n",
    "for section in paper[\"sections\"]:\n",
    "    response = chain.run({\n",
    "        \"preparation_instructions\": prep_instructions,\n",
    "        \"subject\": paper[\"subject\"],\n",
    "        \"pdf_data\": pdf_data,\n",
    "        \"title\": section[\"title\"],\n",
    "        \"description\": section[\"description\"],\n",
    "    })\n",
    "    gen_sections.append({\"title\": section[\"title\"], \"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gen_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = {\"sections\": gen_sections}\n",
    "with open(\"generated20241213\", \"w\") as f:\n",
    "    yaml.dump(dump, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protoyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../drivers/chromedriver'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 179\u001b[0m\n\u001b[1;32m    172\u001b[0m     dump_generated_sections({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msections\u001b[39m\u001b[38;5;124m\"\u001b[39m: paper_content}, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout/lastgeneration.dump\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    173\u001b[0m     save_latex_sections(\n\u001b[1;32m    174\u001b[0m         tex_template_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../templates/paper_template.tex\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    175\u001b[0m         sections\u001b[38;5;241m=\u001b[39mpaper_content,\n\u001b[1;32m    176\u001b[0m         outpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout/lastgenerated.tex\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    177\u001b[0m     )\n\u001b[0;32m--> 179\u001b[0m main()\n",
      "Cell \u001b[0;32mIn[2], line 143\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m pdf_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../refexamples/ArigaK2023_Langmuir.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../refexamples/FangC_ApplicationsLangmuir.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../refexamples/ArigaK2022_PastAndFutureLangmuir.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# \"../refexamples/LuC2024_AIScientist.pdf\"\u001b[39;00m\n\u001b[1;32m    141\u001b[0m ]\n\u001b[1;32m    142\u001b[0m paper_subject \u001b[38;5;241m=\u001b[39m paper_cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 143\u001b[0m paper_structure \u001b[38;5;241m=\u001b[39m generate_paper_structure(\n\u001b[1;32m    144\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mpaper_cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgen_struct_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    145\u001b[0m     subject\u001b[38;5;241m=\u001b[39mpaper_subject,\n\u001b[1;32m    146\u001b[0m     pdf_paths\u001b[38;5;241m=\u001b[39mpdf_paths,\n\u001b[1;32m    147\u001b[0m     outfile\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenstruct.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m )\n\u001b[1;32m    150\u001b[0m ctx \u001b[38;5;241m=\u001b[39m setup_context_msg(\n\u001b[1;32m    151\u001b[0m     response_fmt_prompt\u001b[38;5;241m=\u001b[39mpaper_cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    152\u001b[0m     pdf_paths\u001b[38;5;241m=\u001b[39mpdf_paths,\n\u001b[1;32m    153\u001b[0m )\n\u001b[1;32m    154\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatGoogleGenerativeAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-1.5-pro\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 40\u001b[0m, in \u001b[0;36mgenerate_paper_structure\u001b[0;34m(prompt, subject, pdf_paths, outfile)\u001b[0m\n\u001b[1;32m     37\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{subject}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, subject)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Use NotebookLM bot to send it\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m driver \u001b[38;5;241m=\u001b[39m init_driver(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../drivers/chromedriver\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m nblm \u001b[38;5;241m=\u001b[39m NotebookLMBot(\n\u001b[1;32m     42\u001b[0m     user\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNBLM_EMAIL\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     43\u001b[0m     password\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNBLM_PASSWORD\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     44\u001b[0m     driver\u001b[38;5;241m=\u001b[39mdriver,\n\u001b[1;32m     45\u001b[0m     src_paths\u001b[38;5;241m=\u001b[39mpdf_paths\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nblm\u001b[38;5;241m.\u001b[39mlogin():\n",
      "Cell \u001b[0;32mIn[2], line 20\u001b[0m, in \u001b[0;36minit_driver\u001b[0;34m(browser_path, driver_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m op\u001b[38;5;241m.\u001b[39madd_experimental_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetach\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m op\u001b[38;5;241m.\u001b[39madd_experimental_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexcludeSwitches\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable-logging\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 20\u001b[0m driver \u001b[38;5;241m=\u001b[39m uc\u001b[38;5;241m.\u001b[39mChrome(\n\u001b[1;32m     21\u001b[0m         chrome_options\u001b[38;5;241m=\u001b[39mop,\n\u001b[1;32m     22\u001b[0m         browser_executable_path\u001b[38;5;241m=\u001b[39mbrowser_path,\n\u001b[1;32m     23\u001b[0m         driver_executable_path\u001b[38;5;241m=\u001b[39mdriver_path\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m driver\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/undetected_chromedriver/__init__.py:258\u001b[0m, in \u001b[0;36mChrome.__init__\u001b[0;34m(self, options, user_data_dir, driver_executable_path, browser_executable_path, port, enable_cdp_events, desired_capabilities, advanced_elements, keep_alive, log_level, headless, version_main, patcher_force_close, suppress_welcome, use_subprocess, debug, no_sandbox, user_multi_procs, **kw)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatcher \u001b[38;5;241m=\u001b[39m Patcher(\n\u001b[1;32m    252\u001b[0m     executable_path\u001b[38;5;241m=\u001b[39mdriver_executable_path,\n\u001b[1;32m    253\u001b[0m     force\u001b[38;5;241m=\u001b[39mpatcher_force_close,\n\u001b[1;32m    254\u001b[0m     version_main\u001b[38;5;241m=\u001b[39mversion_main,\n\u001b[1;32m    255\u001b[0m     user_multi_procs\u001b[38;5;241m=\u001b[39muser_multi_procs,\n\u001b[1;32m    256\u001b[0m )\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# self.patcher.auto(user_multiprocess = user_multi_num_procs)\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatcher\u001b[38;5;241m.\u001b[39mauto()\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# self.patcher = patcher\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/undetected_chromedriver/patcher.py:150\u001b[0m, in \u001b[0;36mPatcher.auto\u001b[0;34m(self, executable_path, force, version_main, _)\u001b[0m\n\u001b[1;32m    148\u001b[0m ispatched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_binary_patched(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutable_path)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ispatched:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_exe()\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/undetected_chromedriver/patcher.py:347\u001b[0m, in \u001b[0;36mPatcher.patch_exe\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m    346\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatching driver executable \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutable_path)\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutable_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+b\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh:\n\u001b[1;32m    348\u001b[0m     content \u001b[38;5;241m=\u001b[39m fh\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;66;03m# match_injected_codeblock = re.search(rb\"{window.*;}\", content)\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../drivers/chromedriver'"
     ]
    }
   ],
   "source": [
    "from typing import List,Union\n",
    "import undetected_chromedriver as uc\n",
    "from fake_useragent import UserAgent\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_core.messages import SystemMessage, AIMessage\n",
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from chatbots import NotebookLMBot\n",
    "\n",
    "import re\n",
    "import os\n",
    "from time import sleep\n",
    "\n",
    "def init_driver(browser_path: Union[str,None] = None, driver_path: Union[str,None] = None) -> uc.Chrome:\n",
    "    op = uc.ChromeOptions()\n",
    "    op.add_argument(f\"user-agent={UserAgent.random}\")\n",
    "    op.add_argument(\"user-data-dir=./\")\n",
    "    op.add_experimental_option(\"detach\", True)\n",
    "    op.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "    driver = uc.Chrome(\n",
    "            chrome_options=op,\n",
    "            browser_executable_path=browser_path,\n",
    "            driver_executable_path=driver_path\n",
    "    )\n",
    "    return driver\n",
    "\n",
    "def get_pdf_contents(pdf_paths: List[str]):\n",
    "    doc_data = []\n",
    "    for file in pdf_paths:\n",
    "        loader = PyPDFLoader(file)\n",
    "        doc_data.extend(loader.load())\n",
    "    return doc_data\n",
    "\n",
    "def generate_paper_structure(prompt: str, subject: str, pdf_paths: List[str], outfile: str, driver_path: str = \"../drivers/chromedriver\", browser_path: Union[str,None] = None):\n",
    "    \"\"\" Generate paper structure using NotebookLM \"\"\"\n",
    "    if prompt.find(\"{subject}\") != -1:\n",
    "        prompt = prompt.replace(\"{subject}\", subject)\n",
    "\n",
    "    # Use NotebookLM bot to send it\n",
    "    driver = init_driver(browser_path, driver_path)\n",
    "    nblm = NotebookLMBot(\n",
    "        user=os.environ[\"NBLM_EMAIL\"],\n",
    "        password=os.environ[\"NBLM_PASSWORD\"],\n",
    "        driver=driver,\n",
    "        src_paths=pdf_paths\n",
    "    )\n",
    "    if not nblm.login():\n",
    "        print(\"Unable to login to NotebookLM\")\n",
    "        return\n",
    "    \n",
    "    nblm.send_prompt(prompt, sleep_for=30)\n",
    "    response = nblm.get_last_response()\n",
    "\n",
    "    # format response and save to yaml\n",
    "    result = \"sections:\\n\"\n",
    "    for line in response.split(\"\\n\"):\n",
    "        result += f\"  {line}\\n\"\n",
    "    \n",
    "    with open(outfile, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(result)\n",
    "\n",
    "    driver.quit()\n",
    "    return read_yaml(outfile)\n",
    "\n",
    "def setup_context_msg(response_fmt_prompt: str, pdf_paths: List[str]):\n",
    "    \"\"\" Setup context SystemMessage with writing instructions + PDFs contents \"\"\"\n",
    "    \n",
    "    context = response_fmt_prompt\n",
    "    pdf_content = get_pdf_contents(pdf_paths)\n",
    "    context += \"\\n\\nThe PDF content of the given references are:\\n\"\n",
    "    context += \"\\n\".join([doc.page_content for doc in pdf_content])\n",
    "\n",
    "    return SystemMessage(content=context)\n",
    "\n",
    "def init_chain(llm, ctx_msg: SystemMessage, write_prompt: str):\n",
    "    \"\"\" Setup LLMChain with proper prompts and context \"\"\"\n",
    "    req_prompt = HumanMessagePromptTemplate.from_template(write_prompt)\n",
    "    \n",
    "    full_prompt = ChatPromptTemplate.from_messages([ctx_msg, req_prompt])\n",
    "    chain = full_prompt | llm\n",
    "    return full_prompt, chain\n",
    "\n",
    "def write_section(chain, subject: str, title: str, description: str) -> AIMessage:\n",
    "    \"\"\" Write the given section \"\"\"\n",
    "    return chain.invoke({\n",
    "        \"subject\": subject,\n",
    "        \"title\": title,\n",
    "        \"description\": description,\n",
    "    })\n",
    "\n",
    "def dump_generated_sections(sections: dict, outpath: str):\n",
    "    with open(outpath, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.safe_dump(sections, f)\n",
    "\n",
    "\n",
    "def save_latex_sections(tex_template_path: str, sections: List[dict], outpath: str):\n",
    "    \"\"\" \n",
    "    Join the contents of every section to the output LaTeX file \n",
    "    'sections' must be a list of dictionaries with two keys: 'title' and 'content'\n",
    "    \"\"\"\n",
    "    with open(tex_template_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        tex_template = f.read()\n",
    "\n",
    "    paper_content = \"\"\n",
    "    \n",
    "    bib_content = \"\"\n",
    "    bib_pattern = r\"\\\\begin{filecontents\\*}(.*?)\\\\end{filecontents\\*}\"\n",
    "\n",
    "    for section in sections:\n",
    "        # Extract biblatex file content\n",
    "        match = re.search(bib_pattern, section[\"content\"], re.DOTALL)\n",
    "        sec_bib_content = match.group(1).strip() if match else None\n",
    "        if sec_bib_content is None:\n",
    "            print(\"FAILED TO MATCH BIBLATEX CONTENT IN SECTION:\", section[\"title\"])\n",
    "            continue\n",
    "\n",
    "        section_text = re.sub(bib_pattern, \"\", section[\"content\"], flags=re.DOTALL)\n",
    "        \n",
    "        paper_content += section_text\n",
    "        bib_content += sec_bib_content\n",
    "    bib_content = bib_content.replace(\"{mybib.bib}\", \"\")\n",
    "    bib_file = outpath+\"bib.bib\"\n",
    "    \n",
    "    # Replace paper content in latex template and save it\n",
    "    tex_content = tex_template.replace(\"{content}\", paper_content).replace(\"{bibresourcefile}\", os.path.basename(bib_file))\n",
    "    with open(outpath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(tex_content)\n",
    "\n",
    "    # also save the biblatex file\n",
    "    with open(bib_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(bib_content)\n",
    "\n",
    "\n",
    "def main():\n",
    "    paper_cfg = read_yaml(\"../templates/prompt_config.yaml\")\n",
    "    pdf_paths = [\n",
    "        \"../refexamples/ArigaK2023_Langmuir.pdf\",\n",
    "        \"../refexamples/FangC_ApplicationsLangmuir.pdf\",\n",
    "        \"../refexamples/ArigaK2022_PastAndFutureLangmuir.pdf\",\n",
    "        # \"../refexamples/LuC2024_AIScientist.pdf\"\n",
    "    ]\n",
    "    paper_subject = paper_cfg[\"subject\"]\n",
    "    paper_structure = generate_paper_structure(\n",
    "        prompt=paper_cfg[\"gen_struct_prompt\"],\n",
    "        subject=paper_subject,\n",
    "        pdf_paths=pdf_paths,\n",
    "        outfile=\"genstruct.yaml\",\n",
    "    )\n",
    "\n",
    "    ctx = setup_context_msg(\n",
    "        response_fmt_prompt=paper_cfg[\"response_format\"],\n",
    "        pdf_paths=pdf_paths,\n",
    "    )\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "    prompt, chain = init_chain(llm, ctx, paper_cfg[\"write_prompt\"])\n",
    "\n",
    "    paper_content = []\n",
    "    for section in paper_structure[\"sections\"]:\n",
    "        airesponse = write_section(chain, paper_subject, section[\"title\"], section[\"description\"])\n",
    "        paper_content.append(\n",
    "            {\n",
    "                \"title\": section[\"title\"],\n",
    "                \"content\": airesponse.content,\n",
    "            }\n",
    "        )\n",
    "        print(\"====> FINISHED WRITING SECTION:\", section[\"title\"])\n",
    "        print(\"====> REPONSE METADATA:\", airesponse.usage_metadata)\n",
    "        # wait because of gemini-1.5-pro quota (2 RPM, 32000 TPM)\n",
    "        sleep(60*1.5)\n",
    "\n",
    "    os.makedirs(\"out\", exist_ok=True)\n",
    "    dump_generated_sections({\"sections\": paper_content}, \"out/lastgeneration.dump\")\n",
    "    save_latex_sections(\n",
    "        tex_template_path=\"../templates/paper_template.tex\",\n",
    "        sections=paper_content,\n",
    "        outpath=\"out/lastgenerated.tex\",\n",
    "    )\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([sec[\"title\"] for sec in read_yaml(\"genstruct.yaml\")[\"sections\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
