{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====> ENVIRONMENT SETUP\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "def read_yaml(fpath: str) -> dict:\n",
    "    with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return data\n",
    "\n",
    "def abs_join(par: str, path: str) -> str:\n",
    "    return os.path.abspath(os.path.join(par, path))\n",
    "\n",
    "CREDENTIALS = read_yaml(\"../credentials.yaml\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = CREDENTIALS[\"google_key\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = CREDENTIALS[\"openai_key\"]\n",
    "os.environ[\"NBLM_EMAIL\"] = CREDENTIALS[\"nblm_email\"]\n",
    "os.environ[\"NBLM_PASSWORD\"] = CREDENTIALS[\"nblm_password\"]\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========> COMMON VARIABLES\n",
    "BROWSER_PATH = \"C:/Users/jcmcs/AppData/Local/BraveSoftware/Brave-Browser/Application/brave.exe\"\n",
    "DRIVER_PATH = \"../drivers/chromedriver.exe\"\n",
    "\n",
    "TEX_TEMPLATE_PATH = \"../templates/paper_template.tex\"\n",
    "PROMPT_CONFIG_PATH = \"../templates/prompt_config.yaml\"\n",
    "REVISION_CONFIG_PATH = \"../templates/review_config.yaml\"\n",
    "\n",
    "CWD = os.getcwd()\n",
    "OUT_DIR = abs_join(CWD, \"out\")\n",
    "OUT_GEN_STRUCTURE_PATH = abs_join(OUT_DIR, \"genstruct.yaml\")\n",
    "OUT_TEX_PATH = abs_join(OUT_DIR, \"lastgenerated.tex\")\n",
    "OUT_DUMP_PATH = abs_join(OUT_DIR, \"lastgenerated.dump\")\n",
    "OUT_REVIEWED_TEX_PATH = abs_join(OUT_DIR, \"reviewed-\"+os.path.basename(OUT_TEX_PATH))\n",
    "OUT_REVIEWED_DUMP_PATH = abs_join(OUT_DIR, \"reviewed-\"+os.path.basename(OUT_DUMP_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protoyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List,Union\n",
    "import undetected_chromedriver as uc\n",
    "from fake_useragent import UserAgent\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_core.messages import SystemMessage, AIMessage\n",
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from chatbots import NotebookLMBot\n",
    "\n",
    "import re\n",
    "import os\n",
    "from time import sleep\n",
    "\n",
    "def countdown_print(msg: str, sec: int):\n",
    "    print(msg, end=' ')\n",
    "    print(\"0 s\", end='\\r')\n",
    "    for t in range(sec):\n",
    "        sleep(1)\n",
    "        print(f\"{msg} {t} s\", end='\\r')\n",
    "\n",
    "def init_driver(browser_path: Union[str,None] = None, driver_path: Union[str,None] = None) -> uc.Chrome:\n",
    "    op = uc.ChromeOptions()\n",
    "    op.add_argument(f\"user-agent={UserAgent.random}\")\n",
    "    op.add_argument(\"user-data-dir=./\")\n",
    "    op.add_experimental_option(\"detach\", True)\n",
    "    op.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "    driver = uc.Chrome(\n",
    "        chrome_options=op,\n",
    "        browser_executable_path=browser_path,\n",
    "        driver_executable_path=driver_path\n",
    "    )\n",
    "    return driver\n",
    "\n",
    "def get_pdf_contents(pdf_paths: List[str]):\n",
    "    doc_data = []\n",
    "    for file in pdf_paths:\n",
    "        loader = PyPDFLoader(file)\n",
    "        doc_data.extend(loader.load())\n",
    "    return doc_data\n",
    "\n",
    "def generate_paper_structure(nblm: NotebookLMBot, prompt: str, subject: str, pdf_paths: List[str], outfile: str, driver_path: str = \"../drivers/chromedriver\", browser_path: Union[str,None] = None):\n",
    "    \"\"\" Generate paper structure using NotebookLM \"\"\"\n",
    "    if prompt.find(\"{subject}\") != -1:\n",
    "        prompt = prompt.replace(\"{subject}\", subject)\n",
    "\n",
    "    # Use NotebookLM bot to send it\n",
    "    nblm.send_prompt(prompt, sleep_for=40)\n",
    "    response = nblm.get_last_response()\n",
    "\n",
    "    # format response and save to yaml\n",
    "    result = \"sections:\\n\"\n",
    "    for line in response.split(\"\\n\"):\n",
    "        result += f\"  {line}\\n\"\n",
    "    \n",
    "    with open(outfile, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(result)\n",
    "\n",
    "    result = read_yaml(outfile)\n",
    "    print(f\"Finished generating paper structure. Got a structure with {len(result[\"sections\"])} sections.\\n\")\n",
    "\n",
    "    return read_yaml(outfile)\n",
    "\n",
    "def setup_context_msg(\n",
    "        header_prompt: str, \n",
    "        pdf_paths: List[str], \n",
    "        summarize = False,\n",
    "        summary_llm = None,\n",
    "    ):\n",
    "    \"\"\" Setup context SystemMessage with writing instructions + PDFs contents \"\"\"\n",
    "    \n",
    "    context = header_prompt\n",
    "    pdf_content = get_pdf_contents(pdf_paths)\n",
    "\n",
    "    ctx_content = \"\"\n",
    "    if summarize and (summary_llm is not None):\n",
    "        # Summarize pdf contents\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "        chunks = text_splitter.split_documents(pdf_content)\n",
    "        summaries = []\n",
    "        for chunk in chunks:\n",
    "            summary = summary_llm(f\"Summarize this text: {chunk}\")\n",
    "            sleep(1*60)\n",
    "            summaries.append(summary)\n",
    "        ctx_content = \"\\n\".join(summaries)\n",
    "    else:\n",
    "        # Include entire PDF content\n",
    "        contents = [doc.page_content for doc in pdf_content]\n",
    "        ctx_content = \"\\n\".join(contents)\n",
    "\n",
    "    context += \"\\n\\nThe PDF content of the given references are:\\n\" + ctx_content\n",
    "\n",
    "    return SystemMessage(content=context)\n",
    "\n",
    "def vector_store_from_pdf_content(pdf_content, txtembed_model: str = \"google\"):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_documents(pdf_content)\n",
    "    chunks = [chunk.page_content for chunk in chunks]\n",
    "\n",
    "    match txtembed_model.strip().lower():\n",
    "        case \"openai\":\n",
    "            embeddings = OpenAIEmbeddings()\n",
    "        case \"google\":\n",
    "            embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "        case _:\n",
    "            raise ValueError(f\"Valid txtembed_model values are 'openai' or 'google'\")\n",
    "    vector_store = FAISS.from_texts(chunks, embeddings)\n",
    "    return vector_store\n",
    "\n",
    "def setup_ctx_msg_faiss(header_prompt: str, vector_store: FAISS, section_query: str, k: int = 5):\n",
    "    context = header_prompt\n",
    "    \n",
    "    relevant_docs = vector_store.similarity_search(section_query, k=k)\n",
    "    context += \"\\n\"\n",
    "    context += \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    return context\n",
    "\n",
    "def init_chain(llm, ctx_msg: SystemMessage, write_prompt: str):\n",
    "    \"\"\" Setup LLMChain with proper prompts and context \"\"\"\n",
    "    req_prompt = HumanMessagePromptTemplate.from_template(write_prompt)\n",
    "    \n",
    "    full_prompt = ChatPromptTemplate.from_messages([ctx_msg, req_prompt])\n",
    "    \n",
    "    chain = full_prompt | llm\n",
    "    return full_prompt, chain\n",
    "\n",
    "def write_section(chain, subject: str, title: str, description: str) -> AIMessage:\n",
    "    \"\"\" Write the given section \"\"\"\n",
    "    return chain.invoke({\n",
    "        \"subject\": subject,\n",
    "        \"title\": title,\n",
    "        \"description\": description,\n",
    "    })\n",
    "\n",
    "def dump_generated_sections(sections: dict, outpath: str):\n",
    "    with open(outpath, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.safe_dump(sections, f)\n",
    "\n",
    "\n",
    "def save_latex_sections(tex_template_path: str, sections: List[dict], outpath: str):\n",
    "    \"\"\" \n",
    "    Join the contents of every section to the output LaTeX file \n",
    "    'sections' must be a list of dictionaries with two keys: 'title' and 'content'\n",
    "    \"\"\"\n",
    "    with open(tex_template_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        tex_template = f.read()\n",
    "\n",
    "    paper_content = \"\"\n",
    "    \n",
    "    bib_content = \"\"\n",
    "    bib_pattern = r\"\\\\begin{filecontents\\*}(.*?)\\\\end{filecontents\\*}\"\n",
    "\n",
    "    for section in sections:\n",
    "        # Extract biblatex file content\n",
    "        match = re.search(bib_pattern, section[\"content\"], re.DOTALL)\n",
    "        sec_bib_content = match.group(1).strip() if match else None\n",
    "        if sec_bib_content is None:\n",
    "            print(\"FAILED TO MATCH BIBLATEX CONTENT IN SECTION:\", section[\"title\"])\n",
    "            continue\n",
    "\n",
    "        section_text = re.sub(bib_pattern, \"\", section[\"content\"], flags=re.DOTALL)\n",
    "        \n",
    "        paper_content += section_text\n",
    "        bib_content += sec_bib_content\n",
    "    bib_content = bib_content.replace(\"{mybib.bib}\", \"\")\n",
    "    bib_file = outpath+\"bib.bib\"\n",
    "    \n",
    "    # Replace paper content in latex template and save it\n",
    "    tex_content = tex_template.replace(\"{content}\", paper_content).replace(\"{bibresourcefile}\", os.path.basename(bib_file))\n",
    "    tex_content = tex_content.replace(\"```latex\", \"\").replace(\"```\",\"\")\n",
    "    with open(outpath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(tex_content)\n",
    "\n",
    "    # also save the biblatex file\n",
    "    with open(bib_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(bib_content)\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "paper_cfg = read_yaml(PROMPT_CONFIG_PATH)\n",
    "pdf_paths = [\n",
    "    \"../refexamples/ArigaK2023_ChemOfMat.pdf\",\n",
    "    \"../refexamples/FangC2022_LangmuirBattery.pdf\",\n",
    "    \"../refexamples/OliveiraO2022_PastAndFuture.pdf\",\n",
    "    # \"../refexamples/LuC2024_AIScientist.pdf\"\n",
    "]\n",
    "paper_subject = paper_cfg[\"subject\"]\n",
    "\n",
    "driver = init_driver(BROWSER_PATH, DRIVER_PATH)\n",
    "nblm = NotebookLMBot(\n",
    "    user=os.environ[\"NBLM_EMAIL\"],\n",
    "    password=os.environ[\"NBLM_PASSWORD\"],\n",
    "    driver=driver,\n",
    "    src_paths=pdf_paths,\n",
    ")\n",
    "if not nblm.login():\n",
    "    print(\"Unable to login to NotebookLM\")\n",
    "    exit()\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "paper_structure = generate_paper_structure(\n",
    "    nblm=nblm,\n",
    "    prompt=paper_cfg[\"gen_struct_prompt\"],\n",
    "    subject=paper_subject,\n",
    "    pdf_paths=pdf_paths,\n",
    "    outfile=OUT_GEN_STRUCTURE_PATH,\n",
    "    driver_path=DRIVER_PATH,\n",
    "    browser_path=BROWSER_PATH,\n",
    ")\n",
    "# paper_structure = read_yaml(OUT_GEN_STRUCTURE_PATH)\n",
    "\n",
    "ctx = setup_context_msg(\n",
    "    header_prompt=paper_cfg[\"response_format\"],\n",
    "    pdf_paths=pdf_paths,\n",
    "    summarize=False,\n",
    "    #summary_llm=ChatOpenAI(model=\"chatgpt-4o-latest\")\n",
    ")\n",
    "\n",
    "#llm = ChatOpenAI(model=\"o1-preview\")\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "_, chain = init_chain(llm, ctx, paper_cfg[\"write_prompt\"])\n",
    "\n",
    "# vecstore = vector_store_from_pdf_content(\n",
    "#     pdf_content=get_pdf_contents(pdf_paths),\n",
    "#     txtembed_model=\"google\"\n",
    "# )\n",
    "\n",
    "paper_content = []\n",
    "for i, section in enumerate(paper_structure[\"sections\"]):\n",
    "    print(f\"====> STARTED WRITING SECTION: {section[\"title\"]} ({i+1}/{len(paper_structure[\"sections\"])})\")\n",
    "\n",
    "    airesponse = write_section(chain, paper_subject, section[\"title\"], section[\"description\"])\n",
    "    paper_content.append(\n",
    "        {\n",
    "            \"title\": section[\"title\"],\n",
    "            \"content\": airesponse.content,\n",
    "        }\n",
    "    )\n",
    "    print(\"====> FINISHED WRITING SECTION:\", section[\"title\"])\n",
    "    print(\"====> REPONSE METADATA:\", airesponse.usage_metadata)\n",
    "    dump_generated_sections({\"sections\": paper_content}, OUT_DUMP_PATH)\n",
    "\n",
    "    cooldown_sec = int(60*1.5)\n",
    "    print(f\"Cooldown of {cooldown_sec} s because of request limitations...\\n\")\n",
    "    # wait because of TPM (request quota)\n",
    "    countdown_print(\"Countdown: \", cooldown_sec)\n",
    "\n",
    "save_latex_sections(\n",
    "    tex_template_path=TEX_TEMPLATE_PATH,\n",
    "    sections=paper_content,\n",
    "    outpath=OUT_TEX_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Revision process\n",
    "rev_config = read_yaml(REVISION_CONFIG_PATH)\n",
    "\n",
    "with open(OUT_TEX_PATH, \"r\", encoding=\"utf-8\") as f, open(OUT_TEX_PATH+\"bib.bib\", \"r\", encoding=\"utf-8\") as bf:\n",
    "    latex_content = f.read()\n",
    "    bib_content = bf.read()\n",
    "\n",
    "# First copy to temporary .txt files (can only send .txt to NotebookLM)\n",
    "tmp_tex = OUT_TEX_PATH.replace(\".tex\", \".txt\")\n",
    "tmp_bib = OUT_TEX_PATH+\"bib.txt\"\n",
    "with open(tmp_tex, \"w\", encoding=\"utf-8\") as tmptex_f, open(tmp_bib, \"w\", encoding=\"utf-8\") as tmpbib_f:\n",
    "    tmptex_f.write(latex_content)\n",
    "    tmpbib_f.write(bib_content)\n",
    "\n",
    "# add files as sources\n",
    "nblm.append_sources([tmp_tex, tmp_bib], sleep_for=20)\n",
    "os.remove(tmp_tex)\n",
    "os.remove(tmp_bib)\n",
    "\n",
    "# split paper content\n",
    "pattern = r\"(\\\\section\\{.*?\\}.*?)(?=(\\\\section\\{|\\\\printbibliography|\\\\end\\{document\\}))\"\n",
    "matches = re.findall(pattern, latex_content, re.DOTALL)\n",
    "\n",
    "# Process matches into a list of dictionaries\n",
    "sections = []\n",
    "for match, _ in matches:\n",
    "    # Extract title from the section line\n",
    "    title_match = re.search(r\"\\\\section\\{(.*?)\\}\", match)\n",
    "    title = title_match.group(1).strip() if title_match else \"Unknown\"\n",
    "    sections.append({\"title\": title, \"content\": match.strip()})\n",
    "\n",
    "\n",
    "# Improve each section\n",
    "improved = []\n",
    "rev_ctx = setup_context_msg(\n",
    "    header_prompt=\"The PDF content of the references is:\",\n",
    "    pdf_paths=pdf_paths,\n",
    "    summarize=False,\n",
    ")\n",
    "_, chain = init_chain(llm, rev_ctx, rev_config[\"improve_prompt\"])\n",
    "for i, section in enumerate(sections):\n",
    "    print(f\"====> STARTED REVIEW FOR SECTION {section['title']} ({i+1}/{len(sections)})\")\n",
    "\n",
    "    # ask points for notebooklm\n",
    "    nblm_prompt = rev_config[\"nblm_point_prompt\"].replace(\"{generatedpaperfile}\", tmp_tex).replace(\"{number}\", str(i+1)).replace(\"{title}\", section[\"title\"])\n",
    "    nblm.send_prompt(nblm_prompt, sleep_for=40)\n",
    "    improv_points = nblm.get_last_response()\n",
    "    improv_points = improv_points[improv_points.find(\"Clarity and Coherence\"):]\n",
    "\n",
    "    print(\"Sending improvement prompt to LLM...\")\n",
    "    airesponse = chain.invoke({\n",
    "        \"title\": section[\"title\"],\n",
    "        \"sectionlatex\": section[\"content\"],\n",
    "        \"sectionimprovement\": improv_points,\n",
    "        \"biblatex\": bib_content,\n",
    "    })\n",
    "    improved.append({\n",
    "        \"title\": section[\"title\"],\n",
    "        \"content\": airesponse.content,\n",
    "    })\n",
    "    print(\"====> FINISHED REVIEWING SECTION:\", section[\"title\"])\n",
    "    print(\"====> REPONSE METADATA:\", airesponse.usage_metadata, \"\\n\")\n",
    "    dump_generated_sections({\"sections\": improved}, OUT_REVIEWED_DUMP_PATH)\n",
    "    cooldown_sec = int(60*1.5)\n",
    "    print(f\"Cooldown of {cooldown_sec} s because of request limitations...\")\n",
    "    # wait because of TPM (request quota)\n",
    "    countdown_print(\"Countdown:\", cooldown_sec)\n",
    "\n",
    "save_latex_sections(\n",
    "    tex_template_path=TEX_TEMPLATE_PATH,\n",
    "    sections=improved,\n",
    "    outpath=OUT_REVIEWED_TEX_PATH,\n",
    ")\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(improved)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
